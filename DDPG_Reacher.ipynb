{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "---\n",
    "In this notebook, we train DDPG with the Reacher environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment for training\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Train the Agent with DDPG\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  Alternatively, you can skip to the next code cell to load the pre-trained weights from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benhosken/training/udacity/deep_rl_nanodegree/deep-reinforcement-learning/p2_continuous-control/ddpg_agent.py:117: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.70\tScore: 0.70 New best\n",
      "Episode 8\tAverage Score: 0.83\tScore: 2.22 New best\n",
      "Episode 9\tAverage Score: 1.08\tScore: 3.03 New best\n",
      "Episode 12\tAverage Score: 1.14\tScore: 2.46 New best\n",
      "Episode 13\tAverage Score: 1.24\tScore: 2.35 New best\n",
      "Episode 14\tAverage Score: 1.42\tScore: 3.78 New best\n",
      "Episode 16\tAverage Score: 1.65\tScore: 5.20 New best\n",
      "Episode 17\tAverage Score: 1.68\tScore: 2.07 New best\n",
      "Episode 18\tAverage Score: 1.72\tScore: 2.49 New best\n",
      "Episode 19\tAverage Score: 1.76\tScore: 2.47 New best\n",
      "Episode 20\tAverage Score: 1.88\tScore: 4.17 New best\n",
      "Episode 21\tAverage Score: 1.96\tScore: 3.55 New best\n",
      "Episode 22\tAverage Score: 1.98\tScore: 2.36 New best\n",
      "Episode 23\tAverage Score: 2.06\tScore: 3.75 New best\n",
      "Episode 24\tAverage Score: 2.11\tScore: 3.33 New best\n",
      "Episode 25\tAverage Score: 2.23\tScore: 4.99 New best\n",
      "Episode 26\tAverage Score: 2.38\tScore: 6.15 New best\n",
      "Episode 27\tAverage Score: 2.41\tScore: 3.28 New best\n",
      "Episode 28\tAverage Score: 2.55\tScore: 6.23 New best\n",
      "Episode 29\tAverage Score: 2.61\tScore: 4.42 New best\n",
      "Episode 30\tAverage Score: 2.70\tScore: 5.29 New best\n",
      "Episode 31\tAverage Score: 2.74\tScore: 3.98 New best\n",
      "Episode 32\tAverage Score: 2.81\tScore: 5.03 New best\n",
      "Episode 33\tAverage Score: 2.93\tScore: 6.58 New best\n",
      "Episode 34\tAverage Score: 3.11\tScore: 9.25 New best\n",
      "Episode 35\tAverage Score: 3.17\tScore: 5.00 New best\n",
      "Episode 36\tAverage Score: 3.33\tScore: 9.04 New best\n",
      "Episode 37\tAverage Score: 3.42\tScore: 6.69 New best\n",
      "Episode 38\tAverage Score: 3.58\tScore: 9.55 New best\n",
      "Episode 39\tAverage Score: 3.71\tScore: 8.71 New best\n",
      "Episode 40\tAverage Score: 3.87\tScore: 9.95 New best\n",
      "Episode 41\tAverage Score: 3.96\tScore: 7.61 New best\n",
      "Episode 42\tAverage Score: 4.05\tScore: 7.89 New best\n",
      "Episode 43\tAverage Score: 4.23\tScore: 11.72 New best\n",
      "Episode 44\tAverage Score: 4.40\tScore: 11.70 New best\n",
      "Episode 45\tAverage Score: 4.57\tScore: 11.90 New best\n",
      "Episode 46\tAverage Score: 4.64\tScore: 7.95 New best\n",
      "Episode 47\tAverage Score: 4.79\tScore: 11.75 New best\n",
      "Episode 48\tAverage Score: 4.95\tScore: 12.29 New best\n",
      "Episode 49\tAverage Score: 5.20\tScore: 16.96 New best\n",
      "Episode 50\tAverage Score: 5.30\tScore: 10.58 New best\n",
      "Episode 51\tAverage Score: 5.48\tScore: 14.39 New best\n",
      "Episode 52\tAverage Score: 5.71\tScore: 17.55 New best\n",
      "Episode 53\tAverage Score: 5.88\tScore: 14.64 New best\n",
      "Episode 54\tAverage Score: 5.99\tScore: 11.94 New best\n",
      "Episode 55\tAverage Score: 6.23\tScore: 19.12 New best\n",
      "Episode 56\tAverage Score: 6.49\tScore: 20.52 New best\n",
      "Episode 57\tAverage Score: 6.73\tScore: 20.36 New best\n",
      "Episode 58\tAverage Score: 7.00\tScore: 22.11 New best\n",
      "Episode 59\tAverage Score: 7.06\tScore: 11.04 New best\n",
      "Episode 60\tAverage Score: 7.24\tScore: 17.67 New best\n",
      "Episode 61\tAverage Score: 7.52\tScore: 24.54 New best\n",
      "Episode 62\tAverage Score: 7.77\tScore: 22.56 New best\n",
      "Episode 63\tAverage Score: 8.02\tScore: 23.82 New best\n",
      "Episode 64\tAverage Score: 8.29\tScore: 25.47 New best\n",
      "Episode 65\tAverage Score: 8.64\tScore: 30.73 New best\n",
      "Episode 66\tAverage Score: 8.83\tScore: 21.38 New best\n",
      "Episode 67\tAverage Score: 8.92\tScore: 14.37 New best\n",
      "Episode 68\tAverage Score: 9.23\tScore: 30.23 New best\n",
      "Episode 69\tAverage Score: 9.47\tScore: 25.59 New best\n",
      "Episode 70\tAverage Score: 9.67\tScore: 23.79 New best\n",
      "Episode 71\tAverage Score: 9.96\tScore: 30.26 New best\n",
      "Episode 72\tAverage Score: 10.30\tScore: 34.44 New best\n",
      "Episode 73\tAverage Score: 10.57\tScore: 30.07 New best\n",
      "Episode 74\tAverage Score: 10.91\tScore: 35.47 New best\n",
      "Episode 75\tAverage Score: 11.09\tScore: 24.58 New best\n",
      "Episode 76\tAverage Score: 11.40\tScore: 34.50 New best\n",
      "Episode 77\tAverage Score: 11.67\tScore: 31.98 New best\n",
      "Episode 78\tAverage Score: 12.01\tScore: 38.14 New best\n",
      "Episode 79\tAverage Score: 12.21\tScore: 27.95 New best\n",
      "Episode 80\tAverage Score: 12.47\tScore: 33.14 New best\n",
      "Episode 81\tAverage Score: 12.67\tScore: 28.88 New best\n",
      "Episode 82\tAverage Score: 12.98\tScore: 38.30 New best\n",
      "Episode 83\tAverage Score: 13.29\tScore: 38.44 New best\n",
      "Episode 84\tAverage Score: 13.51\tScore: 31.43 New best\n",
      "Episode 85\tAverage Score: 13.74\tScore: 33.73 New best\n",
      "Episode 86\tAverage Score: 14.02\tScore: 37.10 New best\n",
      "Episode 87\tAverage Score: 14.26\tScore: 35.26 New best\n",
      "Episode 88\tAverage Score: 14.53\tScore: 38.08 New best\n",
      "Episode 89\tAverage Score: 14.57\tScore: 17.81 New best\n",
      "Episode 90\tAverage Score: 14.79\tScore: 35.01 New best\n",
      "Episode 91\tAverage Score: 15.00\tScore: 33.37 New best\n",
      "Episode 92\tAverage Score: 15.15\tScore: 28.63 New best\n",
      "Episode 93\tAverage Score: 15.37\tScore: 36.01 New best\n",
      "Episode 94\tAverage Score: 15.47\tScore: 24.93 New best\n",
      "Episode 95\tAverage Score: 15.69\tScore: 35.70 New best\n",
      "Episode 96\tAverage Score: 15.89\tScore: 34.80 New best\n",
      "Episode 97\tAverage Score: 16.06\tScore: 33.20 New best\n",
      "Episode 98\tAverage Score: 16.23\tScore: 32.62 New best\n",
      "Episode 99\tAverage Score: 16.45\tScore: 37.42 New best\n",
      "Episode 100\tAverage Score: 16.68\tScore: 39.52 New best\n",
      "Episode 100\tAverage Score: 16.68\n",
      "Episode 101\tAverage Score: 17.01\tScore: 33.76 New best\n",
      "Episode 102\tAverage Score: 17.38\tScore: 36.94 New best\n",
      "Episode 103\tAverage Score: 17.75\tScore: 38.32 New best\n",
      "Episode 104\tAverage Score: 18.10\tScore: 35.86 New best\n",
      "Episode 105\tAverage Score: 18.47\tScore: 37.35 New best\n",
      "Episode 106\tAverage Score: 18.85\tScore: 38.98 New best\n",
      "Episode 107\tAverage Score: 19.24\tScore: 39.51 New best\n",
      "Episode 108\tAverage Score: 19.57\tScore: 35.04 New best\n",
      "Episode 109\tAverage Score: 19.92\tScore: 37.89 New best\n",
      "Episode 110\tAverage Score: 20.30\tScore: 39.37 New best\n",
      "Episode 111\tAverage Score: 20.67\tScore: 37.33 New best\n",
      "Episode 112\tAverage Score: 21.02\tScore: 37.68 New best\n",
      "Episode 113\tAverage Score: 21.33\tScore: 33.60 New best\n",
      "Episode 114\tAverage Score: 21.68\tScore: 39.00 New best\n",
      "Episode 115\tAverage Score: 22.06\tScore: 38.93 New best\n",
      "Episode 116\tAverage Score: 22.39\tScore: 38.45 New best\n",
      "Episode 117\tAverage Score: 22.77\tScore: 39.58 New best\n",
      "Episode 118\tAverage Score: 23.13\tScore: 38.23 New best\n",
      "Episode 119\tAverage Score: 23.49\tScore: 38.99 New best\n",
      "Episode 120\tAverage Score: 23.83\tScore: 38.35 New best\n",
      "Episode 121\tAverage Score: 24.18\tScore: 38.08 New best\n",
      "Episode 122\tAverage Score: 24.55\tScore: 39.44 New best\n",
      "Episode 123\tAverage Score: 24.88\tScore: 37.39 New best\n",
      "Episode 124\tAverage Score: 25.25\tScore: 39.52 New best\n",
      "Episode 125\tAverage Score: 25.54\tScore: 34.36 New best\n",
      "Episode 126\tAverage Score: 25.85\tScore: 37.54 New best\n",
      "Episode 127\tAverage Score: 26.20\tScore: 38.06 New best\n",
      "Episode 128\tAverage Score: 26.53\tScore: 39.39 New best\n",
      "Episode 129\tAverage Score: 26.84\tScore: 35.00 New best\n",
      "Episode 130\tAverage Score: 27.17\tScore: 38.77 New best\n",
      "Episode 131\tAverage Score: 27.48\tScore: 34.27 New best\n",
      "Episode 132\tAverage Score: 27.80\tScore: 36.88 New best\n",
      "Episode 133\tAverage Score: 28.12\tScore: 39.22 New best\n",
      "Episode 134\tAverage Score: 28.41\tScore: 37.72 New best\n",
      "Episode 135\tAverage Score: 28.75\tScore: 39.31 New best\n",
      "Episode 136\tAverage Score: 29.02\tScore: 36.25 New best\n",
      "Episode 137\tAverage Score: 29.35\tScore: 39.45 New best\n",
      "Episode 138\tAverage Score: 29.64\tScore: 39.09 New best\n"
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    max_score = -np.Inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations \n",
    "\n",
    "        agent.reset()\n",
    "        agent_scores = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=False)         # select an action (preclipped)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            agent_scores += env_info.rewards                         # update the score (for each agent)\n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "            \n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "            if np.any(dones):\n",
    "                break \n",
    "\n",
    "        mean_score = np.mean(agent_scores) # calculate the mean score across the agents\n",
    "        scores_deque.append(mean_score)\n",
    "        scores.append(mean_score)\n",
    "        average_score = np.mean(scores_deque)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, average_score, mean_score), end=\"\")\n",
    "        if average_score > max_score:\n",
    "            print(\" New best\")\n",
    "            max_score = average_score\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score)) \n",
    "        if average_score > 30:\n",
    "            print('\\nEpisode {}: Avg. Score {:.2f} > 30.'.format(i_episode, average_score))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break # as we have solvced it            \n",
    "    return scores\n",
    "\n",
    "\n",
    "# reload the best weights so far\n",
    "# agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "# agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Watch the trained Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the scores (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states)                         # select an action (preclipped)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
